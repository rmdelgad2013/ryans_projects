{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagged Trees Classifier\n",
    "\n",
    "Bagging (short for boostrat-aggregating) is an ensembling method that aggregates the predictions of many weak learners to make a stronger supervised machine learning model. It's commonly used with decision trees, as they're weak learners with high variance. This model, the BaggedTreesClassifier, is trained by iteratively \n",
    "1. Randomly subsampling the observations in the dataset\n",
    "2. Training a DecisionTreeClassifier on the subsample\n",
    "3. Storing the DecisionTreeClassifier object in an array \n",
    "\n",
    "The model predicts new observations by\n",
    "1. Predidcting the outcome using each tree in the array\n",
    "2. Using a majority vote method to determine the bagged classifier's prediction (for integer encoded labels, it uses the mode function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggedTreesClassifier:\n",
    "    \n",
    "    def __init__(self, n_estimators, n_samples):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.trees = []\n",
    "        self.n_samples = n_samples\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.n_estimators):         \n",
    "            # Randomly subsample observation-wise and feature-wise\n",
    "            sample_inds = np.random.choice(X.shape[0], size=self.n_samples, replace=False)\n",
    "            X_sample = X[sample_inds,:]\n",
    "            y_sample = y[sample_inds]\n",
    "    \n",
    "            # Fit the Decision Tree on the subsampled data\n",
    "            tree = DecisionTreeClassifier()\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        all_tree_predictions = np.array([t.predict(X) for t in self.trees])\n",
    "        forest_predictions = np.squeeze(mode(all_tree_predictions, axis=0).mode)\n",
    "        return forest_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BaggedTreesClassifier(n_estimators=100, n_samples=30)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9666666666666667\n",
      "Confusion matrix:\n",
      "[[ 8  0  0]\n",
      " [ 0 12  0]\n",
      " [ 0  1  9]]\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score: {}'.format(accuracy_score(y_pred, y_test)))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(y_pred, y_test))\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

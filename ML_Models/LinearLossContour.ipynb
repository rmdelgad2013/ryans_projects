{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import load_iris, load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe I'll come back and fix the contour plot some day..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up: Add a bias term to X, initialize the parameters to random values, and set the learning rate & number of iterations\n",
    "X_b = np.hstack([np.ones((100,1)), X])  # add a column of ones to X so we also calculate an intercept\n",
    "theta = np.random.rand(2,1)\n",
    "eta = 0.1\n",
    "n_iter = 20\n",
    "\n",
    "thetas = []\n",
    "mses = []\n",
    "y_preds = []\n",
    "\n",
    "f, ((ax1, ax2), (ax3, _)) = plt.subplots(2, 2)\n",
    "f.set_size_inches(12, 12)\n",
    "\n",
    "# First plot: scatter plot of X & y, with fitted lines that turn more redd with each iteration\n",
    "ax1.scatter(X, y, c='g')\n",
    "ax1.set_title('Scatter of X and y')\n",
    "ax1.set_ylabel('y')\n",
    "ax2.set_xlabel('X')\n",
    "\n",
    "# Set up the colour map for the fitted lines\n",
    "cm = plt.get_cmap('bwr')\n",
    "ax1.set_color_cycle([cm(i/n_iter) for i in range(n_iter)])\n",
    "\n",
    "for i in range(n_iter):\n",
    "    \n",
    "    # Calculate the gradient and update the parameters\n",
    "    gradient = (2/X.shape[0]) * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta -= eta * gradient\n",
    "    \n",
    "    # Calculate the predicted values for all observations, then calculate the MSE\n",
    "    y_pred = X_b.dot(theta)\n",
    "    mse = np.sum(((y_pred - y) ** 2)) / X.shape[0]  # ToDo: revisit this calculation\n",
    "    \n",
    "    # Plot the fitted line for this iteration in the first plot\n",
    "    ax1.plot(X, y_pred)\n",
    "    \n",
    "    # Save the calculated values\n",
    "    mses.append(copy(mse))\n",
    "    thetas.append(copy(theta))\n",
    "    y_preds.append(copy(y_pred))\n",
    "    \n",
    "# Second plot: The MSE with each iteration\n",
    "ax2.plot(mses)\n",
    "ax2.set_title('MSE with each iteration')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('MSE')\n",
    "\n",
    "# Third plot: contour plot of the loss function\n",
    "thetas = np.squeeze(thetas)\n",
    "x_lin = np.linspace(np.rint(np.min(thetas[:,0])), np.rint(np.max(thetas[:,0])))\n",
    "y_lin = np.linspace(np.rint(np.min(thetas[:,1])), np.rint(np.max(thetas[:,1])))\n",
    "\n",
    "x_lin, y_lin = np.meshgrid(x_lin, y_lin)\n",
    "xy = np.array([x_lin.ravel(), y_lin.ravel()]).T\n",
    "mse_lin = np.sum(X_b.dot(xy.T) - np.outer(y, np.ones(2500)), axis=0) / X.shape[0]\n",
    "mse_lin = mse_lin.reshape(x_lin.shape)\n",
    "\n",
    "ax3.contour(x_lin, y_lin, mse_lin, norm=LogNorm(vmin=1.0, vmax=1000.0), \n",
    "            levels=np.logspace(0, 3, 20))\n",
    "\n",
    "f.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

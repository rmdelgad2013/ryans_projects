{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors Classifier\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a simple \"model-free\" approach to classification and regression. For classification, it's successful where the decision boundary between classes is not smooth. \"Fitting\" the model involves no work at all, as KNN uses the training data to make predictions. For predicting label classes, the model works by \n",
    "1. Calculating the (usually Euclidian) distance of the target observation's feature space relative to each observation in the training dataset.\n",
    "2. Determine the labels of the K closest observations\n",
    "3. Choose the mode of those K observations as our predicted label.\n",
    "\n",
    "For regression, we use the label mean instead of the model. Because prediction is done using the training data, it scales linearly with both the number of training observations and the number of dimensions. Prediction, however, scales with the number of training observations and dimensions. It's also sub-optimal in high dimension feature spaces beause all training vectors are almost equidistant (assuming a Euclidian distance function) from the test vector. Thus, plain KNN is best for situations with smaller, low-dimensionality datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My KNNClassifier is inspired by the KNeighborsClassifier in scikit-learn. I'll build mine using NumPy arrays and the scipy.stats.mode function. For simplicity, I'll limit the scope of the model to classification, and I'll use only Euclidian distance as my distance calculation function. I'll evaluate model using standard functions from sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My *fit* method is to just read the training set's feature vectors and labels into instance variables. My *predict* method uses this algorithm to predict new data:\n",
    "    1. Populate y_pred with empty values.\n",
    "    2. For each obs (row) in X_pred,\n",
    "        a. Determine the distances of the obs from all of the observations in self.X\n",
    "        b. Sort all of the distances\n",
    "        c. Subset the first k distances (i.e. the lowest k distances)\n",
    "        d. Find the mode of those k distances\n",
    "        e. Append the mode to y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KNNClassifier:\n",
    "    def __init__(self, k=1):\n",
    "        self.k = k\n",
    "\n",
    "    def euclid_distance(self, target, other):\n",
    "        '''\n",
    "        Calculates the Euclidian distance of the target (1xd) array against the other array. \n",
    "        \"other\" could be a single 1xd array, or it could be an nxd matrix.\n",
    "        '''\n",
    "        \n",
    "        return np.linalg.norm(target - other, axis=1)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        \"Fits\" the model. Since this is a lazy evaluator, this will just entail reading the \n",
    "        training data into self.X & y, so that we can use them in the predict function.\n",
    "        '''\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def predict(self, X_pred):\n",
    "        '''\n",
    "        Predicts the outcome variable y from inputs X_pred. X_pred is a nxd array where n \n",
    "        is the number of observations and d is the number of features. returns y_pred, an \n",
    "        nx1 array of predicted outcomes.            \n",
    "        '''\n",
    "        \n",
    "        n_obs = X_pred.shape[0]\n",
    "        y_pred = np.empty(n_obs)\n",
    "\n",
    "        for i in range(n_obs):\n",
    "            distances = self.euclid_distance(X_pred[i], self.X)\n",
    "            y_sorted  = self.y[distances.argsort()]\n",
    "            neighbors = y_sorted[:self.k]\n",
    "            y_pred[i] = stats.mode(neighbors)[0][0]\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the iris dataset, and split it into randomized train and test data 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a KNNClassifier object with k=5, and fit it to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn = KNNClassifier(k=5)\n",
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll evaluate the model with the test data. I'll calculate the accuracy score and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual & Predicted y values\n",
      "[[ 1.  1.]\n",
      " [ 0.  0.]\n",
      " [ 2.  2.]\n",
      " [ 0.  0.]\n",
      " [ 1.  1.]\n",
      " [ 2.  2.]\n",
      " [ 0.  0.]\n",
      " [ 1.  1.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 1.  1.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]\n",
      " [ 0.  0.]\n",
      " [ 2.  2.]\n",
      " [ 2.  2.]\n",
      " [ 0.  0.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]\n",
      " [ 2.  2.]\n",
      " [ 2.  2.]\n",
      " [ 0.  0.]\n",
      " [ 1.  1.]\n",
      " [ 2.  2.]\n",
      " [ 2.  2.]\n",
      " [ 0.  0.]\n",
      " [ 2.  2.]]\n",
      "\n",
      "\n",
      "Accuracy score: 1.0\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "[[11  0  0]\n",
      " [ 0 10  0]\n",
      " [ 0  0  9]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "print('Actual & Predicted y values')\n",
    "print(np.vstack([y_test, y_pred]).T)\n",
    "print('\\n')\n",
    "\n",
    "knn_accuracy_score = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy score: {}'.format(knn_accuracy_score))\n",
    "print('\\n')\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this particular train/test split, it's 100% accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also verify that my results are similar to sklearn's KNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual & Predicted y values\n",
      "[[1 1]\n",
      " [0 0]\n",
      " [2 2]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [2 2]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [0 0]\n",
      " [2 2]]\n",
      "\n",
      "\n",
      "Accuracy score: 1.0\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "[[11  0  0]\n",
      " [ 0 10  0]\n",
      " [ 0  0  9]]\n"
     ]
    }
   ],
   "source": [
    "sklearn_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "sklearn_knn.fit(X_train, y_train)\n",
    "sklearn_y_pred = sklearn_knn.predict(X_test)\n",
    "\n",
    "print('Actual & Predicted y values')\n",
    "print(np.vstack([y_test, sklearn_y_pred]).T)\n",
    "print('\\n')\n",
    "\n",
    "sklearn_knn_accuracy_score = accuracy_score(y_test, sklearn_y_pred)\n",
    "print('Accuracy score: {}'.format(sklearn_knn_accuracy_score))\n",
    "print('\\n')\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_test, sklearn_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "Naive Bayes is a family of simple, supervised learning algorithms that are based on Bayes' Theorem:\n",
    "\n",
    "$$\n",
    "P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)}\n",
    "$$\n",
    "\n",
    "With multiple features, we generalize this to\n",
    "\n",
    "$$\n",
    "P(Y|x_1, x_2, ..., x_n) = \\frac{P(x_1, x_2, ..., x_n| Y)P(Y)}{P(x_1, x_2, ..., x_n)}\n",
    "$$\n",
    "\n",
    "Naive Bayes assumes that the features are indepenent of each other, i.e.\n",
    "\n",
    "$$\n",
    "P(x_i|Y, x_1, ..., x_{i-1}, x_{i+1}, ..., x_n) = P(x_i|Y)\n",
    "$$\n",
    "\n",
    "Additionally, $P(x_1, x_2, ..., x_n)$ is a constant. We can thus simplify the calculation to\n",
    "$$\n",
    "P(Y|x_1, x_2, ..., x_n) = P(Y)\\prod_{i=1}^nP(x_i,|Y)\n",
    "$$\n",
    "\n",
    "We can predict class labels like so:\n",
    "$$\n",
    "\\hat{y} = \\underset{y}{\\arg\\max } \\space P(y)\\prod_{i=1}^nP(x_i|y)\n",
    "$$\n",
    "\n",
    "To explain this in words, I'll use the classic example of classifying emails as either spam or ham. Our feature space is the entire vocabulary of words that we've seen among the emails in our training dataset, and the feature vector for a particular observation is the vector of counts for each word. Let's say a spam email frequently contains the word \"opportunity\", while ham emails frequently contain the word \"report\". If a particular email contains the word \"opportunity\", then it's likely spam; if it contains \"report\", it's likely ham. Additionally, if the email contains \"opportunity\" more than once, then (in the Multinomial case, at least) the email is even more likely to be spam.\n",
    "\n",
    "For text data, the prior probilities can be calculated with simple word counts:\n",
    "$$\n",
    "P(X|Y) = \\frac{ P(X,Y) } { P(Y) } \\approx \\frac{ \\textrm{Count}(X,Y) } { \\textrm{Count}(Y) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I've implemented a Multinomial Naive Bayes classifier inspired by [sklearn's MultinomialNB model](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html). The model is implemented using numpy. I've also implemented some basic text data cleaning with pandas and feature engineering with sklearn. I wrote the code specifically for text data. I'm not sure it will work with other types of data.\n",
    "\n",
    "Like the sklearn model, I implement fit and predict methods. However, I wrote them in a quirky way because I tried to vectorize as much as possible with NumPy. My fit method works like this:\n",
    "    1. Take in X_train (NxW matrix) and y_train (Nx1 vector)\n",
    "    2. Determine the classes by finding the unique values in y_train\n",
    "    3. Iterate over each class:\n",
    "        a. Create a new vector of 0s and 1s indicating whether an element of y_train equals the target class\n",
    "        b. Find the wordcounts for each observation and word in this class by multiplying the 0s/1s vector by X_train\n",
    "            * X_train is already a matrix of word counts derived using CountVectorizer\n",
    "        c. Add in the Laplace smoothing parameter\n",
    "        d. Find the vector log probabilities for each word by dividings the word counts for each word by the total number of\n",
    "           words for that class\n",
    "        e. Append that vector to a list of vectors\n",
    "    4. Vertically stack the list of log probability vectors, resulting in a WxC matrix (W = # words, C = # classes)\n",
    "\n",
    "My predict method is straightforward:\n",
    "    1. Multiply the X_test matrix by the transpose of the matrix of log probabilities\n",
    "    2. Sum the log probabilities for each class in each observation\n",
    "    3. The predicted class for each observation is the class with the highest log probability\n",
    "    4. Return the vector of predicted classes\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the train and test data into DataFrames. Clean the Tweets - remove punctuation, lowercase the strings, and remove words that are less than 3 characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    df['Tweet_cleaned'] = df['Tweet'].str.replace('[.:?!;,]', ' ').str.lower()  # remove punctuation; lowercase\n",
    "    df['Tweet_cleaned'] = df['Tweet_cleaned'].str.replace('(?:^| )\\w{0,3} ', ' ')  # remove words with 3 characters or less\n",
    "    df = df[['Class', 'Tweet_cleaned']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Tweet_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OTHER</td>\n",
       "      <td>¿en donde esta remontada mandrill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OTHER</td>\n",
       "      <td>@katie_phd alternate 'reproachful mandrill' c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OTHER</td>\n",
       "      <td>@theophani i \"drill\" there it would a picture...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OTHER</td>\n",
       "      <td>“@chrisjboyland baby mandrill paignton 29th ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OTHER</td>\n",
       "      <td>“@missmya #nameanamazingband mandrill ” mint c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class                                      Tweet_cleaned\n",
       "0  OTHER                 ¿en donde esta remontada mandrill \n",
       "1  OTHER   @katie_phd alternate 'reproachful mandrill' c...\n",
       "2  OTHER   @theophani i \"drill\" there it would a picture...\n",
       "3  OTHER  “@chrisjboyland baby mandrill paignton 29th ap...\n",
       "4  OTHER  “@missmya #nameanamazingband mandrill ” mint c..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('traintweets.csv', sep='\\t')\n",
    "train_df = clean_df(train_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Tweet_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>APP</td>\n",
       "      <td>just love @mandrillapp transactional email ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>APP</td>\n",
       "      <td>@rossdeane mind submitting request http //help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>APP</td>\n",
       "      <td>@veroapp chance you'll adding mandrill support...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>APP</td>\n",
       "      <td>@elie__ @camj59 jparle relai smtp million mail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>APP</td>\n",
       "      <td>would like send emails welcome password resets...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Class                                      Tweet_cleaned\n",
       "0   APP  just love @mandrillapp transactional email ser...\n",
       "1   APP  @rossdeane mind submitting request http //help...\n",
       "2   APP  @veroapp chance you'll adding mandrill support...\n",
       "3   APP  @elie__ @camj59 jparle relai smtp million mail...\n",
       "4   APP  would like send emails welcome password resets..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('testtweets.csv', sep='\\t')\n",
    "test_df = clean_df(test_df)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the training and test data into labels and feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer()\n",
    "le = LabelEncoder()\n",
    "\n",
    "X_train = count_vec.fit_transform(train_df['Tweet_cleaned']).todense()\n",
    "y_train = le.fit_transform(train_df['Class'])\n",
    "\n",
    "X_test = count_vec.transform(test_df['Tweet_cleaned']).todense()\n",
    "y_test = le.transform(test_df['Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultinomialNaiveBayes:\n",
    "    \n",
    "    def __init__(self, k=1):\n",
    "        self.k = k  # Laplace smoothing parameter\n",
    "        \n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        # Find the number of observations for each label\n",
    "        classes = np.unique(y_train)\n",
    "        \n",
    "        # Form the NxC matrix of priors (N = # obs, C = # classes)\n",
    "        pxy_vectors = []\n",
    "        for cls in classes:            \n",
    "                        \n",
    "            # Find the observations where the observation's class is equal to the target class.\n",
    "            # Convert from True/False to 1/0 so we can use vector multiplication.\n",
    "            y_train_isclass = (y_train == cls).astype(int)  \n",
    "            \n",
    "            # Multiply the feature matrix by the isclass indicator vector to \n",
    "            # zero-out the observations that aren't labeled the target class.\n",
    "            wordcounts = y_train_isclass * X_train\n",
    "            wordcounts = wordcounts + self.k  # +k for Laplace smoothing\n",
    "            \n",
    "            # Calculate the log probabilities by dividing the wordcounts by the total number\n",
    "            # of words, and taking the natural log of that vector.\n",
    "            log_probs = np.log(wordcounts/np.sum(wordcounts))            \n",
    "            \n",
    "            # Store in the prior vectors list\n",
    "            pxy_vectors.append(log_probs)\n",
    "        \n",
    "        # Combine the list of prior vectors into a NxC matrix of priors.\n",
    "        self.p_xy = np.vstack(pxy_vectors)\n",
    "        \n",
    "    \n",
    "    def predict(self, X_pred):\n",
    "        \n",
    "        # Find the probability that the observation is equal to each class\n",
    "        class_probabilities = X_test * self.p_xy.T\n",
    "        \n",
    "        # Find the maximum class probability as the predicted class\n",
    "        y_pred = np.argmax(class_probabilities, axis=1)\n",
    "        \n",
    "        # Convert the Nx1 \"matrix\" into an ndarray before returning it\n",
    "        y_pred = np.squeeze(np.asarray(y_pred))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model. Predict the test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnb = MultinomialNaiveBayes()\n",
    "mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = mnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.85\n",
      "Confusion matrix:\n",
      "[[9 1]\n",
      " [2 8]]\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score: {score}'.format(score=accuracy_score(y_test, y_pred)))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

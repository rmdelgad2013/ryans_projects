{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "logger = logging.getLogger('word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    '''Utility function for splitting an iterable into an iterable of smaller batches'''\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "def pad_sentence(sentence, window_size=1):\n",
    "    START_TOKEN = '<s> '\n",
    "    END_TOKEN = ' </s>'\n",
    "    padded_sentence = (START_TOKEN * window_size) + sentence + (END_TOKEN * window_size)\n",
    "    return padded_sentence\n",
    "\n",
    "def cbow_preprocess(sequence, window_size=1):\n",
    "    '''\n",
    "    Turns 'the quick brown fox jumped over the lazy dog' into\n",
    "    [[('the', 'brown'), 'quick'],\n",
    "     [('quick', 'fox'), 'brown'],\n",
    "     [('brown', 'jumped'), 'fox'],\n",
    "     ...\n",
    "     \n",
    "     except as NumPy arrays.\n",
    "    '''\n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    for i, token in enumerate(sequence):\n",
    "        if ((i < window_size) | (i > len(sequence) - (window_size + 1))):\n",
    "            pass\n",
    "        else:\n",
    "            previous_words = tuple(sequence[i-window_size:i])\n",
    "            next_words = tuple(sequence[i+1:i+window_size+1])\n",
    "            context = previous_words + next_words\n",
    "            label = token\n",
    "            inputs.append(context)\n",
    "            labels.append(label)\n",
    "\n",
    "    inputs = np.squeeze(np.array(inputs))\n",
    "    labels = np.expand_dims(np.array(labels), 1)\n",
    "    \n",
    "    return inputs, labels\n",
    "\n",
    "def skipgram_preprocess(sequence, window_size=1):\n",
    "    '''\n",
    "    Turns 'the quick brown fox jumped over the lazy dog' into\n",
    "    [('quick', 'the'),\n",
    "     ('quick', 'brown'),\n",
    "     ('brown', 'quick'),\n",
    "     ('brown', 'fox'),\n",
    "     ...\n",
    "     \n",
    "     except as NumPy arrays.\n",
    "    '''\n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    for i, token in enumerate(sequence):\n",
    "        if ((i < window_size) | (i > len(sequence) - (window_size + 1))):\n",
    "            pass\n",
    "        else:        \n",
    "            for input_ in sequence[i-window_size:i]:\n",
    "                inputs.append(input_)\n",
    "                labels.append(token)\n",
    "            for input_ in sequence[i+1:i+window_size+1]:\n",
    "                inputs.append(input_)\n",
    "                labels.append(token)\n",
    "    \n",
    "    inputs = np.squeeze(np.array(inputs))\n",
    "    labels = np.expand_dims(np.array(labels), 1)\n",
    "    \n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    '''\n",
    "    ToDo: Write the docs.\n",
    "    ToDo: Add logging statements.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, texts, embedding_dim=100, window_size=2, architecture='skipgram',\n",
    "                 batch_size=32, num_epochs=1, num_sampled=200):\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.window_size = window_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_sampled = num_sampled\n",
    "        self.padded_texts = [pad_sentence(text, window_size) for text in texts]\n",
    "\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.tokenizer.fit_on_texts(self.padded_texts)\n",
    "        self.tokenized_texts = self.tokenizer.texts_to_sequences(self.padded_texts)\n",
    "        self.vocab_size = len(self.tokenizer.word_index)\n",
    "\n",
    "        self._initialize_tensors()\n",
    "\n",
    "        self.embedding_matrix_numpy = None\n",
    "        self.embedding_dict = None\n",
    "\n",
    "        if architecture == 'cbow':\n",
    "            self.train_op = self._get_skipgram_train_op()\n",
    "            self.preprocessor = cbow_preprocess\n",
    "        else:\n",
    "            self.train_op = self._get_cbow_train_op()\n",
    "            self.preprocessor = skipgram_preprocess\n",
    "\n",
    "    def _initialize_tensors(self):\n",
    "        shape = (self.vocab_size, self.embedding_dim)\n",
    "        self.embedding_matrix_ = tf.get_variable(name='embedding_matrix', initializer=tf.random_uniform(shape, -1.0, 1.0))\n",
    "        \n",
    "        nce_w_init = tf.truncated_normal(shape, stddev=1.0 / (self.vocab_size ** 0.5))\n",
    "        self.nce_weights_ = tf.get_variable(name='nce_weights', initializer=nce_w_init)\n",
    "        self.nce_biases_ = tf.get_variable(name='nce_biases', initializer=tf.zeros([self.vocab_size]))\n",
    "        \n",
    "        self.train_inputs_ = tf.placeholder(tf.int32, shape=[self.batch_size])\n",
    "        self.train_labels_ = tf.placeholder(tf.int32, shape=[self.batch_size, 1])\n",
    "\n",
    "    def _concat_train_data(self, preprocessed_texts):\n",
    "        if self.preprocessor == skipgram_preprocess:\n",
    "            contexts = np.hstack(t[0] for t in preprocessed_texts)\n",
    "        else:\n",
    "            contexts = np.vstack(t[0] for t in preprocessed_texts)\n",
    "        labels = np.vstack(t[1] for t in preprocessed_texts)\n",
    "\n",
    "        return contexts, labels\n",
    "\n",
    "    def _get_skipgram_train_op(self):\n",
    "        '''Graph for the skipgram Word2Vec model.'''\n",
    "\n",
    "        embeddings_ = tf.nn.embedding_lookup(self.embedding_matrix_, self.train_inputs_)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(\n",
    "                weights=self.nce_weights_,\n",
    "                biases=self.nce_biases_,\n",
    "                labels=self.train_labels_,\n",
    "                inputs=embeddings_,\n",
    "                num_sampled=self.num_sampled,\n",
    "                num_classes=self.vocab_size\n",
    "            )\n",
    "        )\n",
    "        train_op = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n",
    "\n",
    "        return train_op\n",
    "\n",
    "    def _get_cbow_train_op(self):\n",
    "        '''Graph for the cbow Word2Vec model.'''\n",
    "\n",
    "        embeddings_ = tf.nn.embedding_lookup(self.embedding_matrix_, self.train_inputs_)\n",
    "        bow = tf.reduce_sum(embeddings_, axis=1)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(\n",
    "                weights=self.nce_weights_,\n",
    "                biases=self.nce_biases_,\n",
    "                labels=self.train_labels_,\n",
    "                inputs=embeddings_,\n",
    "                num_sampled=self.num_sampled,\n",
    "                num_classes=self.vocab_size\n",
    "            )\n",
    "        )\n",
    "        train_op = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n",
    "\n",
    "        return train_op\n",
    "\n",
    "    def train(self, new_text=None):\n",
    "        '''Trains the Word2Vec embeddings.'''\n",
    "\n",
    "        # ToDo: Handle new_text\n",
    "        # Ensure that a list of sentences still results in the expected\n",
    "        # output.\n",
    "        preprocessed_texts = [self.preprocessor(text) for text in self.tokenized_texts]\n",
    "        train_inputs, train_labels = self._concat_train_data(preprocessed_texts)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            for i in range(self.num_epochs):\n",
    "                for inputs, labels in zip(batch(train_inputs, self.batch_size), batch(train_labels, self.batch_size)):\n",
    "                    feed_dict = {\n",
    "                        self.train_inputs_: inputs,\n",
    "                        self.train_labels_: labels\n",
    "                    }\n",
    "                    _ = sess.run(self.train_op, feed_dict=feed_dict)\n",
    "\n",
    "        logger.info('Training finished.')\n",
    "        self.embedding_matrix_numpy = sess.eval(self.embedding_matrix_)\n",
    "\n",
    "    def _build_embedding_dict(self):\n",
    "        word_to_id = self.tokenizer.word_index\n",
    "        id_to_word = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "\n",
    "        embedding_dict = {}\n",
    "        for i in range(self.embedding_matrix_numpy.shape[0]):\n",
    "            word = id_to_word[i]\n",
    "            embedding_dict[word] = self.embedding_matrix_numpy[i, :]\n",
    "\n",
    "        return embedding_dict\n",
    "\n",
    "    @property\n",
    "    def embeddings(self):\n",
    "        if not self.embedding_dict:\n",
    "            self.embedding_dict = self._build_embedding_dict()\n",
    "\n",
    "        return self.embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'the quick brown fox jumped over the lazy dog',\n",
    "    'the slow brown fox snuck under the active dog'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(texts=texts, embedding_dim=10, num_sampled=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.nce_biases_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value nce_biases\n\t [[Node: nce_biases/read = Identity[T=DT_FLOAT, _class=[\"loc:@GradientDescent/update_nce_biases/ScatterSub\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](nce_biases)]]\n\nCaused by op 'nce_biases/read', defined at:\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-571484cffff1>\", line 1, in <module>\n    model = Word2Vec(texts=texts, embedding_dim=10, num_sampled=3)\n  File \"<ipython-input-4-d79a9ff061d4>\", line 22, in __init__\n    self._initialize_tensors()\n  File \"<ipython-input-4-d79a9ff061d4>\", line 40, in _initialize_tensors\n    self.nce_biases_ = tf.get_variable(name='nce_biases', initializer=tf.zeros([self.vocab_size]))\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1317, in get_variable\n    constraint=constraint)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1079, in get_variable\n    constraint=constraint)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 425, in get_variable\n    constraint=constraint)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 394, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 786, in _get_single_variable\n    use_resource=use_resource)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2220, in variable\n    use_resource=use_resource)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2210, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2193, in default_variable_creator\n    constraint=constraint)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 235, in __init__\n    constraint=constraint)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 397, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 142, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3795, in identity\n    \"Identity\", input=input, name=name)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value nce_biases\n\t [[Node: nce_biases/read = Identity[T=DT_FLOAT, _class=[\"loc:@GradientDescent/update_nce_biases/ScatterSub\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](nce_biases)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value nce_biases\n\t [[Node: nce_biases/read = Identity[T=DT_FLOAT, _class=[\"loc:@GradientDescent/update_nce_biases/ScatterSub\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](nce_biases)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-c72315b99576>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-d79a9ff061d4>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, new_text)\u001b[0m\n\u001b[0;32m    105\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_labels_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                     }\n\u001b[1;32m--> 107\u001b[1;33m                     \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training finished.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value nce_biases\n\t [[Node: nce_biases/read = Identity[T=DT_FLOAT, _class=[\"loc:@GradientDescent/update_nce_biases/ScatterSub\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](nce_biases)]]\n\nCaused by op 'nce_biases/read', defined at:\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-571484cffff1>\", line 1, in <module>\n    model = Word2Vec(texts=texts, embedding_dim=10, num_sampled=3)\n  File \"<ipython-input-4-d79a9ff061d4>\", line 22, in __init__\n    self._initialize_tensors()\n  File \"<ipython-input-4-d79a9ff061d4>\", line 40, in _initialize_tensors\n    self.nce_biases_ = tf.get_variable(name='nce_biases', initializer=tf.zeros([self.vocab_size]))\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1317, in get_variable\n    constraint=constraint)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1079, in get_variable\n    constraint=constraint)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 425, in get_variable\n    constraint=constraint)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 394, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 786, in _get_single_variable\n    use_resource=use_resource)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2220, in variable\n    use_resource=use_resource)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2210, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2193, in default_variable_creator\n    constraint=constraint)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 235, in __init__\n    constraint=constraint)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 397, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 142, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3795, in identity\n    \"Identity\", input=input, name=name)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\rmdelgad\\Anaconda3\\envs\\datascience\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value nce_biases\n\t [[Node: nce_biases/read = Identity[T=DT_FLOAT, _class=[\"loc:@GradientDescent/update_nce_biases/ScatterSub\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](nce_biases)]]\n"
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'the quick brown fox jumped over the lazy dog',\n",
    "    'the slow brown fox snuck under the active dog'\n",
    "]\n",
    "\n",
    "embedding_dim=100\n",
    "window_size=2\n",
    "architecture='skipgram'\n",
    "num_epochs=5\n",
    "num_sampled=4\n",
    "batch_size=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_texts = [pad_sentence(text, window_size) for text in texts]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(padded_texts)\n",
    "tokenized_texts = tokenizer.texts_to_sequences(padded_texts)\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "\n",
    "preprocessor = skipgram_preprocess\n",
    "\n",
    "inputs = []\n",
    "labels = []\n",
    "\n",
    "for text in tokenized_texts:\n",
    "    sequence_contexts, sequence_labels = preprocessor(text)\n",
    "    inputs.append(sequence_contexts)\n",
    "    labels.append(sequence_labels)\n",
    "\n",
    "if inputs[0].ndim == 1:\n",
    "    inputs = np.hstack(inputs)\n",
    "else:\n",
    "    inputs = np.vstack(inputs)\n",
    "    \n",
    "labels = np.vstack(labels)\n",
    "\n",
    "embeddings_ = tf.nn.embedding_lookup(embedding_matrix_, inputs_batch)\n",
    "\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.nce_loss(\n",
    "        weights=nce_weights_,\n",
    "        biases=nce_biases_,\n",
    "        labels=labels_,\n",
    "        inputs=embeddings_,\n",
    "        num_sampled=num_sampled,\n",
    "        num_classes=vocab_size\n",
    "   )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_texts = [pad_sentence(text, window_size) for text in texts]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(padded_texts)\n",
    "tokenized_texts = tokenizer.texts_to_sequences(padded_texts)\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "\n",
    "preprocessor = cbow_preprocess\n",
    "\n",
    "inputs = []\n",
    "labels = []\n",
    "\n",
    "for text in tokenized_texts:\n",
    "    sequence_contexts, sequence_labels = preprocessor(text)\n",
    "    inputs.append(sequence_contexts)\n",
    "    labels.append(sequence_labels)\n",
    "\n",
    "if inputs[0].ndim == 1:\n",
    "    inputs = np.hstack(inputs)\n",
    "else:\n",
    "    inputs = np.vstack(inputs)\n",
    "    \n",
    "labels = np.vstack(labels)\n",
    "\n",
    "embeddings_ = tf.nn.embedding_lookup(embedding_matrix_, inputs_batch)\n",
    "bow = tf.reduce_sum(embeddings_, axis=1)\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.nce_loss(\n",
    "        weights=nce_weights_,\n",
    "        biases=nce_biases_,\n",
    "        labels=labels_,\n",
    "        inputs=embeddings_,\n",
    "        num_sampled=num_sampled,\n",
    "        num_classes=vocab_size\n",
    "   )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = skipgram_preprocess\n",
    "\n",
    "preprocessed_texts = [preprocessor(text) for text in tokenized_texts]\n",
    "inputs_labels = list(itertools.chain.from_iterable(preprocessed_texts))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    sess.run(initializer)\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        print(f'Running epoch {i}')\n",
    "        for inputs, labels in batch(inputs_labels, batch_size):\n",
    "            labels_arr = np.reshape(np.array(labels), (batch_size, -1))\n",
    "            inputs_arr = \n",
    "            feed_dict = {\n",
    "                train_inputs_: inputs,\n",
    "                train_labels_: labels_arr\n",
    "            }\n",
    "            _ = sess.run(train_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _concat_train_data(preprocessed_texts):\n",
    "    if preprocessor == skipgram_preprocess:            \n",
    "        contexts = np.hstack(t[0] for t in preprocessed_texts)\n",
    "    else:\n",
    "        contexts = np.vstack(t[0] for t in preprocessed_texts)\n",
    "    labels = np.vstack(t[1] for t in preprocessed_texts)\n",
    "\n",
    "    return contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_texts = [pad_sentence(text, window_size) for text in texts]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(padded_texts)\n",
    "tokenized_texts = tokenizer.texts_to_sequences(padded_texts)\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "\n",
    "preprocessor = cbow_preprocess\n",
    "preprocessed_texts = [preprocessor(t) for t in tokenized_texts]\n",
    "\n",
    "contexts, labels = _concat_train_data(preprocessed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(contexts.shape)\n",
    "print(labels.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Science",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum Likelihood Estimation\n",
    "===\n",
    "_by Ryan Delgado_\n",
    "\n",
    "The purpose of this notebook is to introduce or refresh the concept of Maximum Likelihood Estimation (MLE). MLE is a commonly-used method of estimating parameters in a stastical model. It uses probability and numerical optimization to estimate parameters by answering the question \"What is the most likely true value of the parameters that will result in the observations?\"\n",
    "\n",
    "This notebook uses the Normal Distribution as a basic application of MLE. The notebook assumes that you have at least a basic understanding of\n",
    "+ Probability Theory, specifically joint probability\n",
    "+ The Normal Distribution\n",
    "+ Optimization\n",
    "\n",
    "We'll briefly covers the intution behind MLE, and then apply the intuition to an example with the Normal Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition and Intuition of the Likelihood Function\n",
    "===\n",
    "\n",
    "\n",
    "\n",
    "MLE begins with the Likelihood Function, which is rooted in probability. Recall that the joint probability of events A and B both occurring is the product of their individual probabilities: $P(AB) = P(A)P(B)$. $P(AB)$ is the likelihood of both events A and B occurring, so $P(A)P(B)$ can be thought of as its _ikelihood fucntion_. We can generalize this by saying that the Likelihood Function of a set of a events is the joint probability of the events, or with the function:\n",
    "\n",
    "$$ \\prod_{i=1}^{n} P(x_{i}) $$ \n",
    "\n",
    "for events $ x_{1} $ through $ x_{n} $ where $ P(x_{i}) $ is the probability of event $ x_{i} $ occurring.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application of MLE to the Normal Distribution\n",
    "===\n",
    "\n",
    "Say our model is the normal distribution, and we're fitting the model to a set of observations of a random variable $X$. Recall that the normal distribution is defined by the formula\n",
    "$$ f\\left(X, \\mu, \\sigma^{2}\\right) = \\frac{1}{\\sqrt{2\\sigma^{2}\\pi}} e^-{\\frac{\\left(X-\\mu\\right)^2}{2\\sigma^{2}}}$$ \n",
    "\n",
    "A set of $N$ observations of $X$ is a set of $N$ events, so the probability of observing all of these observations is the likelihood function:\n",
    "\n",
    "$$ \\prod_{i=1}^{N} P(X_{i}) = \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\sigma^{2}\\pi}} e^-{\\frac{\\left(X_{i}-\\mu\\right)^2}{2\\sigma^{2}}}$$ \n",
    "\n",
    "This can be interpreted as the probability of this set of events occurring by sampling from a normally distributed population with unknown parameters $\\mu$ and $\\sigma$. \n",
    "\n",
    "The true values of $\\mu$ and $\\sigma$ are currently unknown and must be estimated from the data. This is where MLE becomes useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use NumPy and SciPy's optimize toolkit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by defining the Normal Distribution in a Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the normal distribution\n",
    "# parameters:\n",
    "#   params - list: contains the mean and std deviation params for the normal distribution\n",
    "#   x - 1d ndarray: contains the observations for the random variable X\n",
    "def normal_dist(params, x):\n",
    "    sig, mu = params\n",
    "    return (1 / np.sqrt(2 * (sig ** 2) * np.pi)) * np.exp((-(x - mu) ** 2) / (2 * (sig ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define the likelihood function as the product the observation probabilities. I've also taken the natural log of the likelihood function so that it's easier to numerically optimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the likelihood function\n",
    "# parameters:\n",
    "#   params - list: contains the mean and std deviation params for the normal distribution\n",
    "#   x_array - 1d ndarray: an array of the sample\n",
    "def norm_log_likelihood_function(params, x_array):\n",
    "     return np.log(np.prod([normal_dist(params, x) for x in x_array]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing the Log Likelihood function is equivalent to optimizing the Likelihood function because the Likelihood function monotonically increases. Let's define a lambda function that we'll optimize. I'll be minimizing the negative because it's numerically easier and standard practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nll = lambda *args: -norm_log_likelihood_function(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a random, normally-distributed sample of 100 observations with a mean of approximately 5 and standard deviation of approximately 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample standard deviation (found via np.std): 4.15397283459\n",
      "Sample mean (found via np.mean): 4.91738993881\n"
     ]
    }
   ],
   "source": [
    "sample = np.random.normal(5, 4, 100)\n",
    "sample_std = np.std(sample)\n",
    "sample_mean = np.mean(sample)\n",
    "\n",
    "\n",
    "print(\"Sample standard deviation (found via np.std): %s\" % str(sample_std))\n",
    "print(\"Sample mean (found via np.mean): %s\" % str(sample_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use <python>scipy.optimize.minimize </python>to estimate the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample standard deviation (found via MLE): 4.15397263173\n",
      "Sample mean (found via MLE): 4.91738989301\n"
     ]
    }
   ],
   "source": [
    "result = minimize(nll, [4,1], args=(sample))\n",
    "mle_std = result[\"x\"][0]\n",
    "mle_mean = result[\"x\"][1]\n",
    "\n",
    "print(\"Sample standard deviation (found via MLE): %s\" % str(mle_std))\n",
    "print(\"Sample mean (found via MLE): %s\" % str(mle_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Application of MLE to Ordinary Least Squares Regression\n",
    "===\n",
    "\n",
    "Ordinary Least Squares (OLS) is a method for estimating the parameters in a linear regression model. A linear regression model is a statistical model used to determine the linear relationship between dependent and independent variables. Example linear regression model:\n",
    "\n",
    "$$ y_{i} = \\beta_{0} + \\beta_{1}x_{i}^{(1)} + \\beta_{2}x_{i}^{(2)} + \\varepsilon_{i} $$\n",
    "\n",
    "Where $y_{i}$ is the dependent variable, $x_{i}^{(*)}$ are the independent variable, and $\\varepsilon_{i}$ is the error term. We can more elegantly represent this using matrix notation:\n",
    "\n",
    "$$ \\large Y  \\normalsize =  \\large X\\beta  \\normalsize +  \\large \\varepsilon $$ \n",
    "\n",
    "\n",
    "$$ \n",
    "\\begin{bmatrix} \n",
    "y_{1} \\\\ \\vdots \\\\ y_{i} \\\\\n",
    "\\end{bmatrix}  =  \n",
    "\\begin{bmatrix} \n",
    "x_{1}^{(1)} & \\dots  & x_{1}^{(n)} \\\\ \n",
    "\\vdots & \\ddots & \\vdots \n",
    "\\\\ x_{i}^{(1)} & \\dots  & x_{i}^{(n)} \\\\ \n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix} \n",
    "\\beta_{1} \\\\ \\vdots \\\\ \\beta_{n} \\\\\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix} \n",
    "\\varepsilon_{1} \\\\ \\vdots \\\\ \\varepsilon_{i} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\beta$ parameters in this model can be estimated using MLE. If we assume that the error term is normally distributed with a mean of zero, our Likelihood function will be\n",
    "\n",
    "$$ \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\sigma^{2}\\pi}} e^-{\\frac{\\left(y_{i}-X_{i}\\beta\\right)^2}{2\\sigma^{2}}} $$\n",
    "\n",
    "where $X_{i}$ is one observation from the independent variables matrix $X$, and $y_{i}$ is one observation of the dependent variable. Now the parameters to estimate include $\\beta$, the vector of  parameters in the linear model, and $\\sigma$, the standard error.\n",
    "\n",
    "Let's slightly redefine our normal distribution to fit this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the function for normally distributed error terms in an OLS model\n",
    "# parameters:\n",
    "#   params - list: contains the 1d ndarray of beta params and std error parameter\n",
    "#   x - 1d ndarray: contains the observation of the independent variables in matrix X\n",
    "#   y - double: contains the observation for the dependent variable y\n",
    "def normal_dist_ols(params, x, y):\n",
    "    sig = params[0]\n",
    "    beta = params[1:]\n",
    "    return (1 / np.sqrt(2 * (sig ** 2) * np.pi)) * np.exp((-(y - np.dot(x,beta)) ** 2) / (2 * (sig ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll generate a random sample of observations for $y$ and $X$. We'll only use one independent variable in $X$ for simplicity, in addition to the intercept column (which is a column of 1s only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n",
      "(100, 2)\n"
     ]
    }
   ],
   "source": [
    "y_array = np.mat(np.random.normal(5,4,100)).T\n",
    "x_matrix = np.mat(np.column_stack((np.ones(100), np.random.normal(5,4,100))))\n",
    "print(y_array.shape)\n",
    "print(x_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analytical derivation of the OLS estimator is:\n",
    "\n",
    "$$ \\large \\beta =  \\left(X^{T}X\\right)^{-1}X^{T}y$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta calculated with OLS: [[ 4.57113746]\n",
      " [ 0.17612323]]\n"
     ]
    }
   ],
   "source": [
    "beta_ols = np.linalg.inv(x_matrix.T * x_matrix) * (x_matrix.T * y_array)\n",
    "print(\"Beta calculated with OLS: %s\" % str(beta_ols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Calculating the error vector: $ \\varepsilon = y - X\\beta $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard error calculated with OLS: 3.66545358683\n"
     ]
    }
   ],
   "source": [
    "error_ols = y_array - (x_matrix*beta_ols)\n",
    "standard_error_ols = np.std(error_ols)\n",
    "print(\"Standard error calculated with OLS: %s\" % str(standard_error_ols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the likelihood function for OLS\n",
    "# parameters:\n",
    "#   params - list: contains the mean and std deviation params for the normal distribution\n",
    "#   x_array - 1d ndarray: an array of the sample\n",
    "def norm_log_likelihood_function_ols(params, x_matrix, y_array):\n",
    "     return np.log(np.prod([normal_dist_ols(params, x, y) for x,y in zip(x_matrix, y_array)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nll = lambda *args: -norm_log_likelihood_function_ols(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard error calculated with MLE: 3.66545355161\n",
      "Beta calculated with MLE: [ 4.57113733  0.17612324]\n"
     ]
    }
   ],
   "source": [
    "result = minimize(nll, [3,1,2], args=(x_matrix,y_array))\n",
    "standard_error_mle = result.x[0]\n",
    "beta_mle = result.x[1:]\n",
    "print(\"Standard error calculated with MLE: %s\" % str(standard_error_mle))\n",
    "print(\"Beta calculated with MLE: %s\" % str(beta_mle))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

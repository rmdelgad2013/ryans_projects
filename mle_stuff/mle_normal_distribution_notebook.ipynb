{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum Likelihood Estimation - An Introduction\n",
    "===\n",
    "\n",
    "The purpose of this notebook is to introduce or refresh the concept of Maximum Likelihood Estimationn (MLE) to readers. MLE is a commonly-used method of estimating parameters in a stastical model. It uses probability and numerical optimization to estimate parameters by answering the question \"What is the most likely true value of the parameters that will result in the observations?\"\n",
    "\n",
    "This notebook uses the Normal Distribution as a basic application of MLE. The notebook assumes that the reader has an understanding of\n",
    "+ Probability Theory, specifically joint probability\n",
    "+ The Normal Distribution\n",
    "+ Optimization\n",
    "\n",
    "The notebook briefly covers the intution behind MLE, and then applies the intuition to an example with the Normal Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition and Intuition of the Likelihood Function\n",
    "===\n",
    "\n",
    "Maximum Likelihood Estimationn (MLE) is a method of estimating parameters in a stastical model. It uses probability and numerical optimization to estimate parameters by answering the question \"What is the most likely true value of the parameters that will result in the observations?\"\n",
    "\n",
    "MLE begins with the likelihood function, which is rooted in probability. Recall that the joint probability of events A and B is the product of their individual probabilities: P(AB) = P(A)P(B). P(AB) is the likelihood of both events A and B occurring, so P(A)P(B) can be thought of as its likelihood fucntion. We can generalize this by saying that the likelihood function of a set of a events is the joint probability, or with the function:\n",
    "\n",
    "$$ \\prod_{i=1}^{n} P(x_{i}) $$ \n",
    "\n",
    "for events $ x_{1} $ through $ x_{n} $ where $ P(x_{i}) $ is the probability of event $ x_{i} $ occurring.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application of MLE to the Normal Distribution\n",
    "===\n",
    "\n",
    "Say our model is the normal distribution, and we're fitting the model to a set of observations of a random variable $X$. Recall that the normal distribution is defined with the formula\n",
    "$$ f\\left(X, \\mu, \\sigma^{2}\\right) = \\frac{1}{\\sqrt{2\\sigma^{2}\\pi}} e^-{\\frac{\\left(X-\\mu\\right)^2}{2\\sigma^{2}}}$$ \n",
    "\n",
    "A set of $N$ observations of $X$ is a set of $N$ events, so the probability of observing all of these observations is the likelihood function:\n",
    "\n",
    "$$ \\prod_{i=1}^{N} P(X_{i}) = \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\sigma^{2}\\pi}} e^-{\\frac{\\left(X_{i}-\\mu\\right)^2}{2\\sigma^{2}}}$$ \n",
    "\n",
    "This can be interpreted as the probability of this set of events occurring by sampling from a normally distributed population with unknown parameters $\\mu$ and $\\sigma$. \n",
    "\n",
    "The true values of $\\mu$ and $\\sigma$ are currently unknown and must be estimated from the data. This is where MLE becomes useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by defining the normal distribution in a Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the normal distribution\n",
    "# parameters:\n",
    "#   params - list: contains the mean and std deviation params for the normal distribution\n",
    "#   x - 1d ndarray: contains the observations for the random variable X\n",
    "def normal_dist(params, x):\n",
    "    sig, mu = params\n",
    "    return (1 / np.sqrt(2 * (sig ** 2) * np.pi)) * np.exp((-(x - mu) ** 2) / (2 * (sig ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define the likelihood function as the product the observation probabilities. I've also taken the natural log of the likelihood function so that it's easier to numerically optimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the likelihood function\n",
    "# parameters:\n",
    "#   params - list: contains the mean and std deviation params for the normal distribution\n",
    "#   x_array - 1d ndarray: an array of the sample\n",
    "def norm_log_likelihood_function(params, x_array):\n",
    "     return np.log(np.prod([normal_dist(params, x) for x in x_array]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing the Log Likelihood function if equivalent to optimizing the Likelihood function because the Likelihood function monotonically increases. Let's define a lambda function that we'll optimize. I'll be minimizing the negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nll = lambda *args: -norm_log_likelihood_function(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a random, normally-distributed sample of observations with a mean of approximately 5 and standard deviation of approximately 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample standard deviation (found via np.std): 3.60004583384\n",
      "Sample mean (found via np.mean): 5.04421523434\n"
     ]
    }
   ],
   "source": [
    "sample = np.random.normal(5, 4, 100)\n",
    "sample_std = np.std(sample)\n",
    "sample_mean = np.mean(sample)\n",
    "\n",
    "\n",
    "print(\"Sample standard deviation (found via np.std): %s\" % str(sample_std))\n",
    "print(\"Sample mean (found via np.mean): %s\" % str(sample_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use <python>scipy.optimize.minimize </python>to estimate the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample standard deviation (found via MLE): 3.60004606311\n",
      "Sample mean (found via MLE): 5.04421502909\n"
     ]
    }
   ],
   "source": [
    "result = minimize(nll, [4,1], args=(sample))\n",
    "mle_std = result[\"x\"][0]\n",
    "mle_mean = result[\"x\"][1]\n",
    "\n",
    "print(\"Sample standard deviation (found via MLE): %s\" % str(mle_std))\n",
    "print(\"Sample mean (found via MLE): %s\" % str(mle_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Conclusion\n",
    "===\n",
    "\n",
    "Blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

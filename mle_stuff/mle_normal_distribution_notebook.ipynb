{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition and Intuition of Maximum Likelihood Estimation\n",
    "===\n",
    "\n",
    "Maximum Likelihood Estimationn (MLE) is a method of estimating parameters in a stastical model. It uses probability and numerical optimization to estimate parameters by answering the question \"What is the most likely true value of the parameters that will result in the observations?\"\n",
    "\n",
    "MLE begins with the likelihood function, which is rooted in probability. Recall that the joint probability of events A and B is the product of their individual probabilities: P(AB) = P(A)P(B). P(AB) is the likelihood of both events A and B occurring, so P(A)P(B) can be thought of as its likelihood fucntion. We can generalize this with the function:\n",
    "\n",
    "[likelihood function about here]\n",
    "\n",
    "Say we're fitting the normal distribution to a set of observations of a random variable X. A set of N observations of X is a set of N events, so the probability of observing all of these observations is the likelihood function:\n",
    "\n",
    "[likelihood function 2 about here]\n",
    "\n",
    "This can be interpreted as the probability of this set of events occurring by sampling from a normally distributed population with unknown parameters mu and sigma. The probability of each individual observation is the normal distribution function:\n",
    "\n",
    "[normal distribution function]\n",
    "\n",
    "The true values of mu and sigma are currently unknown and must be estimated from the data. This is where MLE becomes useful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by defining the normal distribution function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the normal distribution\n",
    "# params:\n",
    "#   params - list: contains the mean and std deviation params for the normal distribution\n",
    "#   x - NumPy array: contains the observations for the random variable X\n",
    "def normal_dist(params, x):\n",
    "    sig, mu = params\n",
    "    return (1 / np.sqrt(2 * (sig ** 2) * np.pi)) * np.exp((-(x - mu) ** 2) / (2 * (sig ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll define the likelihood function as the product the observation probabilities. I've also taken the natural log of the likelihood function so that it's easier to numerically optimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the likelihood function\n",
    "# params:\n",
    "#   x_array - 1xN ndarray; an array of the sample\n",
    "def norm_log_likelihood_function(params, x_array):\n",
    "     return np.log(np.prod([normal_dist(params, x) for x in x_array]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing the Log Likelihood function if equivalent to optimizing the Likelihood function because the Likelihood function monotonically increases. Let's define a lambda function that we'll optimize. I'll be minimizing the negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nll = lambda *args: -norm_log_likelihood_function(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a random, normally-distributed sample of observations with a mean of approximately 5 and standard deviation of approximately 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample standard deviation (found via np.std): 3.60004583384\n",
      "Sample mean (found via np.mean): 5.04421523434\n"
     ]
    }
   ],
   "source": [
    "sample = np.random.normal(5, 4, 100)\n",
    "sample_std = np.std(sample)\n",
    "sample_mean = np.mean(sample)\n",
    "\n",
    "\n",
    "print(\"Sample standard deviation (found via np.std): %s\" % str(sample_std))\n",
    "print(\"Sample mean (found via np.mean): %s\" % str(sample_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use scipy.optimize.minimize to estimate the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample standard deviation (found via MLE): 3.60004606311\n",
      "Sample mean (found via MLE): 5.04421502909\n"
     ]
    }
   ],
   "source": [
    "result = minimize(nll, [4,1], args=(sample))\n",
    "mle_std = result[\"x\"][0]\n",
    "mle_mean = result[\"x\"][1]\n",
    "\n",
    "print(\"Sample standard deviation (found via MLE): %s\" % str(mle_std))\n",
    "print(\"Sample mean (found via MLE): %s\" % str(mle_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
